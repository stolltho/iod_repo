{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nji1a9ULLtCA"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnsX1AWKLtCE"
   },
   "source": [
    "# Lab 8.5: Text Classification\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pm8PttyLtCI"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:33.182995Z",
     "start_time": "2019-06-17T01:38:30.045388Z"
    },
    "id": "EUANiH6zLtCK"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58bUNQA0LtCV"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqU7d_qcLtCX"
   },
   "source": [
    "Sample:\n",
    "\n",
    "    __label__2 Stuning even for the non-gamer: This sound ...\n",
    "    __label__2 The best soundtrack ever to anything.: I'm ...\n",
    "    __label__2 Amazing!: This soundtrack is my favorite m ...\n",
    "    __label__2 Excellent Soundtrack: I truly like this so ...\n",
    "    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n",
    "    __label__2 an absolute masterpiece: I am quite sure a ...\n",
    "    __label__1 Buyer beware: This is a self-published boo ...\n",
    "    . . .\n",
    "    \n",
    "There are only two **labels**:\n",
    "- `__label__1`\n",
    "- `__label__2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:42.024845Z",
     "start_time": "2019-06-17T01:38:41.896098Z"
    },
    "id": "rwWFJprZLtCZ"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "\n",
    "trainDF = pd.read_fwf(\n",
    "    filepath_or_buffer = '../../DATA/corpus.txt',\n",
    "    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n",
    "                (11, 9000) # text: makes the it big enough to get to the end of the line\n",
    "               ], \n",
    "    header = 0,\n",
    "    names = ['label', 'text'],\n",
    "    lineterminator = '\\n'\n",
    ")\n",
    "\n",
    "# convert label from [1, 2] to [0, 1]\n",
    "trainDF['label'] = trainDF['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILVIHomLtCf"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G9_8RbOeLtCh",
    "outputId": "03171e73-f977-4da3-bc96-3b446b69497c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   9999 non-null   int64 \n",
      " 1   text    9999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n",
      "None\n",
      "      label                                               text\n",
      "6410      0  Doesn't fit!: The bolt holes don't line up. Th...\n",
      "2225      0  the movie was better: Usually the book is alwa...\n",
      "5970      0  Embarrassing but No Disrespect meant: I was en...\n",
      "1856      0  Waste of time.: I bought this video while thin...\n",
      "3422      0  I appreciate what he attempted, but...: I appr...\n",
      "7062      1  One Of My Wife's Favorites: My wife really lov...\n",
      "9021      0  Doesn't work! Cheaply made.: Bought this as on...\n",
      "5334      0  Does not follow the book much. Had to insert a...\n",
      "8358      1  a good memory: I got this card and I am comple...\n",
      "9544      1  Works great with Windows 7: Just got this card...\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.info())\n",
    "print(trainDF.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YmYgG2pLtCu"
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j5vErjWFLtCy"
   },
   "outputs": [],
   "source": [
    "## split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainDF['text'], trainDF['label'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nUp6oDOLtC1"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKd9yTnyLtC2"
   },
   "source": [
    "### Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:32.674674Z",
     "start_time": "2019-06-17T01:40:31.098889Z"
    },
    "id": "DU2RqqDjLtC3"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object (bag of words)\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}') # pattern to match all strings and digits (no punctuation)\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJs6al0ILtC5"
   },
   "source": [
    "### TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:36.088730Z",
     "start_time": "2019-06-17T01:40:34.519925Z"
    },
    "id": "myjfdfP_LtC6",
    "outputId": "8bc4d529-1f66-4836-acd1-07633c29fd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:57.505221Z",
     "start_time": "2019-06-17T01:40:49.387393Z"
    },
    "id": "-h16dUaVLtC_",
    "outputId": "3be1f8f5-670e-4249-8522-50509c8898a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:41:10.209071Z",
     "start_time": "2019-06-17T01:40:59.211484Z"
    },
    "id": "Y7rmIt49LtDC",
    "outputId": "8f600e7c-b4df-4d89-bfb9-96c8436aa5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3),\n",
      "                token_pattern='\\\\w{1,}')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stoll\\anaconda3\\envs\\iod_py39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:554: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         token_pattern = r'\\w{1,}',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pck1cuvLtDH"
   },
   "source": [
    "### Text / NLP based features\n",
    "\n",
    "Create some other features.\n",
    "\n",
    "Char_Count = Number of Characters in Text\n",
    "\n",
    "Word Count = Number of Words in Text\n",
    "\n",
    "Word Density = Average Number of Char in Words\n",
    "\n",
    "Punctuation Count = Number of Punctuation in Text\n",
    "\n",
    "Title Word Count = Number of Words in Title\n",
    "\n",
    "Uppercase Word Count = Number of Upperwords in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4jebGm1gLtDH",
    "outputId": "124d1ac4-9b64-414d-e973-9ded78662e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 530 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count'] + 1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(''.join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([w for w in x.split() if w.istitle()]))\n",
    "trainDF['uppercase_word_count'] = trainDF['text'].apply(lambda x: len([w for w in x.split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SZ-wnhIvLtDL",
    "outputId": "69d3e28a-291d-4111-9cbc-cc21c37b9037"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7572</th>\n",
       "      <td>0</td>\n",
       "      <td>Cute but painful: Ugh. I was so excited to get...</td>\n",
       "      <td>387</td>\n",
       "      <td>78</td>\n",
       "      <td>4.898734</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>0</td>\n",
       "      <td>Ida B. Wells: Woman of Courage: I was very eag...</td>\n",
       "      <td>310</td>\n",
       "      <td>58</td>\n",
       "      <td>5.254237</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>0</td>\n",
       "      <td>AWFUL!: This was the worst movie I have ever s...</td>\n",
       "      <td>603</td>\n",
       "      <td>115</td>\n",
       "      <td>5.198276</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>1</td>\n",
       "      <td>Thought-provoking and at times chilling: Many ...</td>\n",
       "      <td>915</td>\n",
       "      <td>164</td>\n",
       "      <td>5.545455</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8753</th>\n",
       "      <td>1</td>\n",
       "      <td>I want to be Shirley Manson!: This group has b...</td>\n",
       "      <td>279</td>\n",
       "      <td>55</td>\n",
       "      <td>4.982143</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  char_count  \\\n",
       "7572      0  Cute but painful: Ugh. I was so excited to get...         387   \n",
       "4821      0  Ida B. Wells: Woman of Courage: I was very eag...         310   \n",
       "7422      0  AWFUL!: This was the worst movie I have ever s...         603   \n",
       "6936      1  Thought-provoking and at times chilling: Many ...         915   \n",
       "8753      1  I want to be Shirley Manson!: This group has b...         279   \n",
       "\n",
       "      word_count  word_density  punctuation_count  title_word_count  \\\n",
       "7572          78      4.898734                 10                10   \n",
       "4821          58      5.254237                 11                11   \n",
       "7422         115      5.198276                 16                13   \n",
       "6936         164      5.545455                 26                11   \n",
       "8753          55      4.982143                  7                 9   \n",
       "\n",
       "      uppercase_word_count  \n",
       "7572                     3  \n",
       "4821                     3  \n",
       "7422                     8  \n",
       "6936                     0  \n",
       "8753                     3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:44:03.442730Z",
     "start_time": "2019-06-17T01:44:02.298791Z"
    },
    "id": "Z-l2iZcLLtDO"
   },
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-9d0G59LtDR"
   },
   "source": [
    "Part of Speech in **SpaCy**\n",
    "\n",
    "    POS   DESCRIPTION               EXAMPLES\n",
    "    ----- ------------------------- ---------------------------------------------\n",
    "    ADJ   adjective                 big, old, green, incomprehensible, first\n",
    "    ADP   adposition                in, to, during\n",
    "    ADV   adverb                    very, tomorrow, down, where, there\n",
    "    AUX   auxiliary                 is, has (done), will (do), should (do)\n",
    "    CONJ  conjunction               and, or, but\n",
    "    CCONJ coordinating conjunction  and, or, but\n",
    "    DET   determiner                a, an, the\n",
    "    INTJ  interjection              psst, ouch, bravo, hello\n",
    "    NOUN  noun                      girl, cat, tree, air, beauty\n",
    "    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n",
    "    PART  particle                  's, not,\n",
    "    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n",
    "    PROPN proper noun               Mary, John, London, NATO, HBO\n",
    "    PUNCT punctuation               ., (, ), ?\n",
    "    SCONJ subordinating conjunction if, while, that\n",
    "    SYM   symbol                    $, %, §, ©, +, −, ×, ÷, =, :), 😝\n",
    "    VERB  verb                      run, runs, running, eat, ate, eating\n",
    "    X     other                     sfpksdpsxmsa\n",
    "    SPACE space\n",
    "    \n",
    "Find out number of Adjective, Adverb, Noun, Numeric, Pronoun, Proposition, Verb.\n",
    "\n",
    "    Hint:\n",
    "    1. Convert text to spacy document\n",
    "    2. Use pos_\n",
    "    3. Use Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:50:15.900377Z",
     "start_time": "2019-06-17T01:50:15.889406Z"
    },
    "id": "NcxmvIOGLtDS"
   },
   "outputs": [],
   "source": [
    "# Initialise some columns for feature counts\n",
    "trainDF['adj_count'] = 0\n",
    "trainDF['adv_count'] = 0\n",
    "trainDF['noun_count'] = 0\n",
    "trainDF['num_count'] = 0\n",
    "trainDF['pron_count'] = 0\n",
    "trainDF['propn_count'] = 0\n",
    "trainDF['verb_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>509</td>\n",
       "      <td>97</td>\n",
       "      <td>5.193878</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>760</td>\n",
       "      <td>129</td>\n",
       "      <td>5.846154</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>743</td>\n",
       "      <td>118</td>\n",
       "      <td>6.243697</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  char_count  \\\n",
       "0      1  The best soundtrack ever to anything.: I'm rea...         509   \n",
       "1      1  Amazing!: This soundtrack is my favorite music...         760   \n",
       "2      1  Excellent Soundtrack: I truly like this soundt...         743   \n",
       "\n",
       "   word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0          97      5.193878                 14                 7   \n",
       "1         129      5.846154                 40                24   \n",
       "2         118      6.243697                 33                52   \n",
       "\n",
       "   uppercase_word_count  adj_count  adv_count  noun_count  num_count  \\\n",
       "0                     3          0          0           0          0   \n",
       "1                     4          0          0           0          0   \n",
       "2                     4          0          0           0          0   \n",
       "\n",
       "   pron_count  propn_count  verb_count  \n",
       "0           0            0           0  \n",
       "1           0            0           0  \n",
       "2           0            0           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hxVRdplBLtDU",
    "outputId": "3bea0c5a-3d73-4750-fcc7-9ece84e8cf7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for each text\n",
    "for i in range(trainDF.shape[0]):\n",
    "    # convert into a spaCy document\n",
    "    doc = nlp(trainDF.iloc[i]['text'])\n",
    "    # initialise feature counters\n",
    "    c = Counter([token.pos_ for token in doc])\n",
    "\n",
    "    trainDF.at[i, 'adj_count'] = c['ADJ']\n",
    "    trainDF.at[i, 'adv_count'] = c['ADV']\n",
    "    trainDF.at[i, 'noun_count'] = c['NOUN']\n",
    "    trainDF.at[i, 'num_count'] = c['NUM']\n",
    "    trainDF.at[i, 'pron_count'] = c['PRON']\n",
    "    trainDF.at[i, 'propn_count'] = c['PROPN']\n",
    "    trainDF.at[i, 'verb_count'] = c['VERB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `at[]` is used to return data in a dataframe at the passed location. The passed location is in the format `[position, Column Name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PROPN': 13,\n",
       "         'CCONJ': 5,\n",
       "         'PUNCT': 12,\n",
       "         'DET': 8,\n",
       "         'NOUN': 15,\n",
       "         'AUX': 9,\n",
       "         'VERB': 10,\n",
       "         'ADV': 4,\n",
       "         'PRON': 17,\n",
       "         'ADJ': 7,\n",
       "         'SCONJ': 1,\n",
       "         'ADP': 14,\n",
       "         'PART': 2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter([token.pos_ for token in doc])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c['ADJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:59:42.424828Z",
     "start_time": "2019-06-17T01:59:42.390920Z"
    },
    "id": "DW1_LKP2LtDX",
    "outputId": "7a5eb5fd-cae1-4e76-f95f-e0fe9f1780d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>326</td>\n",
       "      <td>59</td>\n",
       "      <td>5.433333</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>147</td>\n",
       "      <td>27</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>247</td>\n",
       "      <td>42</td>\n",
       "      <td>5.744186</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>412</td>\n",
       "      <td>83</td>\n",
       "      <td>4.904762</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>546</td>\n",
       "      <td>92</td>\n",
       "      <td>5.870968</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_count  word_count  word_density  punctuation_count  \\\n",
       "1577         326          59      5.433333                  8   \n",
       "1653         147          27      5.250000                  7   \n",
       "2327         247          42      5.744186                  4   \n",
       "3484         412          83      4.904762                 17   \n",
       "1345         546          92      5.870968                 25   \n",
       "\n",
       "      title_word_count  uppercase_word_count  adj_count  adv_count  \\\n",
       "1577                 6                     2          5          3   \n",
       "1653                12                     2          4          0   \n",
       "2327                 7                     1          0          3   \n",
       "3484                10                     4          6          2   \n",
       "1345                 5                     1         16         12   \n",
       "\n",
       "      noun_count  num_count  pron_count  propn_count  verb_count  \n",
       "1577           9          1           5            3           8  \n",
       "1653           3          0           3            4           1  \n",
       "2327           7          0           5            6           5  \n",
       "3484          19          2           6            2           6  \n",
       "1345          18          0          10            1           8  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\n",
    "    'char_count', 'word_count', 'word_density',\n",
    "    'punctuation_count', 'title_word_count',\n",
    "    'uppercase_word_count', 'adj_count',\n",
    "    'adv_count', 'noun_count', 'num_count',\n",
    "    'pron_count', 'propn_count', 'verb_count']\n",
    "\n",
    "trainDF[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQCAUFWYLtDb"
   },
   "source": [
    "### Topic Models as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a popular topic modeling technique to extract topics from a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:27:09.442903Z",
     "start_time": "2019-06-17T02:24:45.531924Z"
    },
    "id": "wg2mAlkRLtDb",
    "outputId": "323bfce4-9263-403a-9214-e9e206d5755f"
   },
   "outputs": [],
   "source": [
    "# Instatiate LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20) # n_components = number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 31661)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train LDA model on BOW\n",
    "X_topics = lda_model.fit_transform(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7999, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0002907 , 0.0002907 , 0.02936047, ..., 0.0002907 , 0.08624418,\n",
       "        0.0002907 ],\n",
       "       [0.00059524, 0.00059524, 0.00059524, ..., 0.00059524, 0.02444083,\n",
       "        0.00059524],\n",
       "       [0.00033557, 0.00033557, 0.00033557, ..., 0.00033557, 0.00033557,\n",
       "        0.00033557],\n",
       "       ...,\n",
       "       [0.00031447, 0.00031447, 0.05691824, ..., 0.00031447, 0.00031447,\n",
       "        0.00660377],\n",
       "       [0.00119048, 0.00119048, 0.00119048, ..., 0.00119048, 0.00119048,\n",
       "        0.00119048],\n",
       "       [0.0004386 , 0.0004386 , 0.01073809, ..., 0.0004386 , 0.0004386 ,\n",
       "        0.0004386 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_topics.shape)\n",
    "X_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 31661)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.05      , 0.05      , 0.05000001, ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       ...,\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.05000001, 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = lda_model.components_ \n",
    "print(topic_word.shape)\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stoll\\anaconda3\\envs\\iod_py39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '00',\n",
       " '000',\n",
       " '001',\n",
       " '002',\n",
       " '00290',\n",
       " '007',\n",
       " '0070412901',\n",
       " '0072316373',\n",
       " '008',\n",
       " '00now',\n",
       " '00yeah',\n",
       " '01',\n",
       " '011',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '052',\n",
       " '06',\n",
       " '06rate',\n",
       " '07',\n",
       " '088',\n",
       " '09',\n",
       " '0s9',\n",
       " '0sx',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000amp',\n",
       " '1000s',\n",
       " '1000uf',\n",
       " '1001',\n",
       " '100m',\n",
       " '100th',\n",
       " '100this',\n",
       " '101',\n",
       " '10162',\n",
       " '102',\n",
       " '1020',\n",
       " '1021',\n",
       " '103',\n",
       " '1030pm',\n",
       " '104',\n",
       " '1048259',\n",
       " '105',\n",
       " '1058',\n",
       " '1059',\n",
       " '107',\n",
       " '108',\n",
       " '1080p',\n",
       " '10bm',\n",
       " '10ft',\n",
       " '10gameplay',\n",
       " '10gb',\n",
       " '10i',\n",
       " '10mb',\n",
       " '10min',\n",
       " '10movie',\n",
       " '10overall',\n",
       " '10p',\n",
       " '10sound',\n",
       " '10th',\n",
       " '10the',\n",
       " '10this',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '110e',\n",
       " '1115',\n",
       " '11223',\n",
       " '114622',\n",
       " '115',\n",
       " '116',\n",
       " '119',\n",
       " '11b',\n",
       " '11g',\n",
       " '11i',\n",
       " '11lbs',\n",
       " '11movie',\n",
       " '11th',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '120100',\n",
       " '120lbs',\n",
       " '120s',\n",
       " '120v',\n",
       " '121',\n",
       " '1215',\n",
       " '123',\n",
       " '125',\n",
       " '128',\n",
       " '128mb',\n",
       " '129',\n",
       " '12th',\n",
       " '12x',\n",
       " '13',\n",
       " '130',\n",
       " '133x',\n",
       " '1340s',\n",
       " '135lbs',\n",
       " '1394',\n",
       " '13th',\n",
       " '13w',\n",
       " '13x',\n",
       " '14',\n",
       " '140',\n",
       " '140lbs',\n",
       " '144',\n",
       " '145',\n",
       " '14th',\n",
       " '14yr',\n",
       " '15',\n",
       " '150',\n",
       " '15000',\n",
       " '1537',\n",
       " '158',\n",
       " '15hrs',\n",
       " '15sci_fi_researcher',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '162',\n",
       " '1630',\n",
       " '165',\n",
       " '168',\n",
       " '169',\n",
       " '16gb',\n",
       " '16mm',\n",
       " '16th',\n",
       " '16v',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '1700s',\n",
       " '171',\n",
       " '172',\n",
       " '175',\n",
       " '1750',\n",
       " '1757',\n",
       " '175lbs',\n",
       " '176',\n",
       " '1775',\n",
       " '1776',\n",
       " '179',\n",
       " '1790',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1800s',\n",
       " '1812',\n",
       " '1814',\n",
       " '1815',\n",
       " '1827',\n",
       " '1830',\n",
       " '1830s',\n",
       " '1834',\n",
       " '1840',\n",
       " '1841',\n",
       " '1850',\n",
       " '186',\n",
       " '1860',\n",
       " '1860s',\n",
       " '1864',\n",
       " '1869',\n",
       " '1870',\n",
       " '1875',\n",
       " '1877',\n",
       " '1879',\n",
       " '1888',\n",
       " '1890',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1900s',\n",
       " '1901',\n",
       " '1905',\n",
       " '1909',\n",
       " '1911',\n",
       " '1912',\n",
       " '1914',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '192',\n",
       " '1920',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1929',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1931',\n",
       " '1932',\n",
       " '1933',\n",
       " '1934',\n",
       " '1935',\n",
       " '1936',\n",
       " '1937',\n",
       " '1938',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1953',\n",
       " '1954',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '199',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1f4t',\n",
       " '1gb',\n",
       " '1ghz',\n",
       " '1hr',\n",
       " '1st',\n",
       " '1x',\n",
       " '1yr',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2004cover',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2011this',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2040s',\n",
       " '2050',\n",
       " '208',\n",
       " '2081',\n",
       " '20battle',\n",
       " '20c',\n",
       " '20fun',\n",
       " '20gb',\n",
       " '20lbs',\n",
       " '20mb',\n",
       " '20min',\n",
       " '20movement',\n",
       " '20mph',\n",
       " '20overall',\n",
       " '20s',\n",
       " '20sound',\n",
       " '20th',\n",
       " '21',\n",
       " '210',\n",
       " '211',\n",
       " '215',\n",
       " '21in',\n",
       " '21st',\n",
       " '22',\n",
       " '2212',\n",
       " '222',\n",
       " '222ms',\n",
       " '224',\n",
       " '22nd',\n",
       " '23',\n",
       " '232',\n",
       " '235am',\n",
       " '23669mlane27716',\n",
       " '23in',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '2400dpi',\n",
       " '240ish',\n",
       " '240v',\n",
       " '243',\n",
       " '244',\n",
       " '2458',\n",
       " '24hrs',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '250lbs',\n",
       " '250mg',\n",
       " '252',\n",
       " '256',\n",
       " '256mb',\n",
       " '256meg',\n",
       " '25cents',\n",
       " '25k',\n",
       " '25movie',\n",
       " '25th',\n",
       " '25v',\n",
       " '25yrs',\n",
       " '26',\n",
       " '262',\n",
       " '264',\n",
       " '265',\n",
       " '266',\n",
       " '26ga',\n",
       " '27',\n",
       " '276',\n",
       " '27th',\n",
       " '28',\n",
       " '280',\n",
       " '28f',\n",
       " '28k',\n",
       " '28th',\n",
       " '29',\n",
       " '29th',\n",
       " '2cnd',\n",
       " '2d',\n",
       " '2gb',\n",
       " '2h',\n",
       " '2hours',\n",
       " '2hrs',\n",
       " '2k',\n",
       " '2k2',\n",
       " '2k3',\n",
       " '2lv',\n",
       " '2lx',\n",
       " '2mhz',\n",
       " '2na',\n",
       " '2nd',\n",
       " '2pac',\n",
       " '2stars',\n",
       " '2though',\n",
       " '2to',\n",
       " '2transformers',\n",
       " '2yr',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '300k',\n",
       " '300lbs',\n",
       " '300watt',\n",
       " '30f',\n",
       " '30pm',\n",
       " '30s',\n",
       " '30th',\n",
       " '31',\n",
       " '310',\n",
       " '3100',\n",
       " '311',\n",
       " '312',\n",
       " '31my',\n",
       " '32',\n",
       " '324',\n",
       " '32bit',\n",
       " '32mb',\n",
       " '33',\n",
       " '33117',\n",
       " '33342',\n",
       " '33x',\n",
       " '34',\n",
       " '342',\n",
       " '34b',\n",
       " '34pm',\n",
       " '34th',\n",
       " '35',\n",
       " '350',\n",
       " '354',\n",
       " '355',\n",
       " '357',\n",
       " '36',\n",
       " '360',\n",
       " '365',\n",
       " '368',\n",
       " '37',\n",
       " '375',\n",
       " '38',\n",
       " '38437',\n",
       " '39',\n",
       " '394',\n",
       " '394mb',\n",
       " '3am',\n",
       " '3d',\n",
       " '3k',\n",
       " '3m',\n",
       " '3months',\n",
       " '3nd',\n",
       " '3rd',\n",
       " '3rds',\n",
       " '3x',\n",
       " '3yo',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '402',\n",
       " '40388',\n",
       " '406',\n",
       " '40619',\n",
       " '40d',\n",
       " '40f',\n",
       " '40g',\n",
       " '40gb',\n",
       " '40in',\n",
       " '40s',\n",
       " '40yr',\n",
       " '41',\n",
       " '4108',\n",
       " '42',\n",
       " '420',\n",
       " '4200',\n",
       " '4255',\n",
       " '42yrs',\n",
       " '43',\n",
       " '4391',\n",
       " '44',\n",
       " '440',\n",
       " '4400',\n",
       " '45',\n",
       " '450',\n",
       " '451',\n",
       " '451at',\n",
       " '451fahrenheit',\n",
       " '454',\n",
       " '457',\n",
       " '45degree',\n",
       " '46',\n",
       " '4600',\n",
       " '4608kbps',\n",
       " '46d',\n",
       " '46movie',\n",
       " '47',\n",
       " '478',\n",
       " '47lw6500',\n",
       " '48',\n",
       " '480',\n",
       " '49',\n",
       " '496',\n",
       " '49th',\n",
       " '4am',\n",
       " '4gb',\n",
       " '4l',\n",
       " '4ounce',\n",
       " '4pin',\n",
       " '4shady',\n",
       " '4star',\n",
       " '4th',\n",
       " '4x',\n",
       " '4x3',\n",
       " '4x6',\n",
       " '4x7',\n",
       " '4yr',\n",
       " '4yrs',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50000',\n",
       " '500megas',\n",
       " '5021',\n",
       " '5022',\n",
       " '5023',\n",
       " '505',\n",
       " '509',\n",
       " '50ish',\n",
       " '50ml',\n",
       " '50running',\n",
       " '50s',\n",
       " '50th',\n",
       " '51',\n",
       " '510',\n",
       " '512',\n",
       " '512mb',\n",
       " '52',\n",
       " '521',\n",
       " '53',\n",
       " '54',\n",
       " '542',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '59523',\n",
       " '5ft',\n",
       " '5hr',\n",
       " '5ish',\n",
       " '5l',\n",
       " '5mb',\n",
       " '5ml',\n",
       " '5mm',\n",
       " '5mutant',\n",
       " '5s',\n",
       " '5th',\n",
       " '5x',\n",
       " '5yo',\n",
       " '5yr',\n",
       " '6',\n",
       " '60',\n",
       " '600',\n",
       " '600mill',\n",
       " '604',\n",
       " '606',\n",
       " '6071',\n",
       " '608',\n",
       " '60gb',\n",
       " '60s',\n",
       " '61',\n",
       " '611',\n",
       " '616',\n",
       " '617727',\n",
       " '62',\n",
       " '620',\n",
       " '625',\n",
       " '63',\n",
       " '633hs',\n",
       " '64',\n",
       " '64mb',\n",
       " '64x2',\n",
       " '65',\n",
       " '650',\n",
       " '65279',\n",
       " '654',\n",
       " '65533',\n",
       " '66',\n",
       " '6620ldg',\n",
       " '66x',\n",
       " '67',\n",
       " '679',\n",
       " '67mm',\n",
       " '689',\n",
       " '69',\n",
       " '690',\n",
       " '69th',\n",
       " '6foot',\n",
       " '6ft',\n",
       " '6th',\n",
       " '6x9',\n",
       " '7',\n",
       " '70',\n",
       " '700',\n",
       " '7000',\n",
       " '702410',\n",
       " '70s',\n",
       " '71',\n",
       " '713',\n",
       " '714',\n",
       " '72',\n",
       " '7200',\n",
       " '720p',\n",
       " '728',\n",
       " '730',\n",
       " '74',\n",
       " '747',\n",
       " '75',\n",
       " '7500',\n",
       " '75k',\n",
       " '76',\n",
       " '7600',\n",
       " '77',\n",
       " '775',\n",
       " '777',\n",
       " '778',\n",
       " '78',\n",
       " '79',\n",
       " '791',\n",
       " '7960',\n",
       " '7am',\n",
       " '7gig',\n",
       " '7i',\n",
       " '7mil',\n",
       " '7r',\n",
       " '7th',\n",
       " '8',\n",
       " '80',\n",
       " '800',\n",
       " '8000',\n",
       " '800kb',\n",
       " '802',\n",
       " '80in',\n",
       " '80mb',\n",
       " '80mm',\n",
       " '80s',\n",
       " '80x40',\n",
       " '8100',\n",
       " '810e',\n",
       " '82',\n",
       " '820',\n",
       " '825c',\n",
       " '83',\n",
       " '85',\n",
       " '8500',\n",
       " '8500dv',\n",
       " '8559',\n",
       " '87',\n",
       " '870',\n",
       " '8701',\n",
       " '88',\n",
       " '883',\n",
       " '888',\n",
       " '89',\n",
       " '897',\n",
       " '8gb',\n",
       " '8ghz',\n",
       " '8in',\n",
       " '8mm',\n",
       " '8th',\n",
       " '8x10',\n",
       " '8yrs',\n",
       " '9',\n",
       " '90',\n",
       " '900',\n",
       " '90000',\n",
       " '90210',\n",
       " '90s',\n",
       " '90th',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '932c',\n",
       " '94',\n",
       " '940',\n",
       " '940c',\n",
       " '94this',\n",
       " '95',\n",
       " '96',\n",
       " '960',\n",
       " '960cse',\n",
       " '961',\n",
       " '97',\n",
       " '9700',\n",
       " '98',\n",
       " '98se',\n",
       " '99',\n",
       " '999',\n",
       " '9999',\n",
       " '9999999999999999999999999999999',\n",
       " '99cents',\n",
       " '99centsand',\n",
       " '99how',\n",
       " '9th',\n",
       " '9v',\n",
       " '_',\n",
       " '_____________edited',\n",
       " '_directly_',\n",
       " '_dwellers',\n",
       " '_foundation_',\n",
       " '_heroes',\n",
       " '_like_',\n",
       " '_might_',\n",
       " '_only_',\n",
       " '_really',\n",
       " '_the',\n",
       " '_think_',\n",
       " 'a',\n",
       " 'a1',\n",
       " 'a10v',\n",
       " 'a1c',\n",
       " 'a201',\n",
       " 'a2dp',\n",
       " 'a40',\n",
       " 'a80',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaarrrggggghhhhhh',\n",
       " 'aaargh',\n",
       " 'aaas',\n",
       " 'aaf',\n",
       " 'aand',\n",
       " 'aare',\n",
       " 'aarrgghh',\n",
       " 'aas',\n",
       " 'aawwww',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandons',\n",
       " 'abarca',\n",
       " 'abated',\n",
       " 'abba',\n",
       " 'abba10',\n",
       " 'abba11',\n",
       " 'abba12',\n",
       " 'abba13',\n",
       " 'abba14',\n",
       " 'abba2',\n",
       " 'abba3',\n",
       " 'abba4',\n",
       " 'abba5',\n",
       " 'abba6',\n",
       " 'abba7',\n",
       " 'abba8',\n",
       " 'abba9',\n",
       " 'abbath',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abble',\n",
       " 'abbott',\n",
       " 'abbreviated',\n",
       " 'abbreviation',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abcfam',\n",
       " 'abcs',\n",
       " 'abd',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abductees',\n",
       " 'abduction',\n",
       " 'abe',\n",
       " 'aber',\n",
       " 'aberaham',\n",
       " 'abfab',\n",
       " 'abiding',\n",
       " 'abierta',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ablum',\n",
       " 'abner',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abomination',\n",
       " 'abore',\n",
       " 'aboriginal',\n",
       " 'aborigine',\n",
       " 'aborted',\n",
       " 'abortion',\n",
       " 'abot',\n",
       " 'abou',\n",
       " 'abound',\n",
       " 'abounding',\n",
       " 'about',\n",
       " 'aboutplan',\n",
       " 'above',\n",
       " 'abracadra',\n",
       " 'abraham',\n",
       " 'abreast',\n",
       " 'abreviated',\n",
       " 'abridged',\n",
       " 'abridgement',\n",
       " 'abridgment',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absense',\n",
       " 'absent',\n",
       " 'absentia',\n",
       " 'absoloutley',\n",
       " 'absolute',\n",
       " 'absolutelly',\n",
       " 'absolutely',\n",
       " 'absolutes',\n",
       " 'absolutle',\n",
       " 'absolutley',\n",
       " 'absolutly',\n",
       " 'absorb',\n",
       " 'absorbant',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorption',\n",
       " 'absoute',\n",
       " 'absoutely',\n",
       " 'abstinance',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'abstracting',\n",
       " 'abstraction',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'aburrido',\n",
       " 'aburridos',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusive',\n",
       " 'abutter',\n",
       " 'abysmal',\n",
       " 'abyss',\n",
       " 'ac',\n",
       " 'ac4',\n",
       " 'aca',\n",
       " 'academa',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academically',\n",
       " 'academics',\n",
       " 'academy',\n",
       " 'acapella',\n",
       " 'acc',\n",
       " 'accadio',\n",
       " 'accedent',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'accelerator',\n",
       " 'accelorator',\n",
       " 'accent',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'accesible',\n",
       " 'access',\n",
       " 'accessary',\n",
       " 'accessed',\n",
       " 'accessible',\n",
       " 'accessibly',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accomidate',\n",
       " 'accommodate',\n",
       " 'accommodates',\n",
       " 'accommodations',\n",
       " 'accomodate',\n",
       " 'accomodates',\n",
       " 'accompanied',\n",
       " 'accompaniment',\n",
       " 'accompaning',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accoount',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accound',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountant',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accredited',\n",
       " 'accross',\n",
       " 'accruing',\n",
       " 'acctually',\n",
       " 'accual',\n",
       " 'accually',\n",
       " 'accumulated',\n",
       " 'accuracy',\n",
       " 'accurage',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accustomed',\n",
       " 'accutate',\n",
       " 'ace',\n",
       " 'acerca',\n",
       " 'aces',\n",
       " 'acesible',\n",
       " 'acessory',\n",
       " 'acetic',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'acheive',\n",
       " 'acheived',\n",
       " 'acheivment',\n",
       " 'aches',\n",
       " 'acheté',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achievers',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achievment',\n",
       " 'achingly',\n",
       " 'achète',\n",
       " 'acid',\n",
       " 'acided',\n",
       " 'acidic',\n",
       " 'acids',\n",
       " 'ack',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'aclass',\n",
       " 'acne',\n",
       " 'acolytes',\n",
       " 'acorn',\n",
       " 'acoustic',\n",
       " 'acquaint',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquaintanceselling',\n",
       " 'acquainted',\n",
       " 'acquiese',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acress',\n",
       " 'acrid',\n",
       " 'acrobat',\n",
       " 'acrobatics',\n",
       " 'acronym',\n",
       " 'across',\n",
       " 'acrylic',\n",
       " 'acsess',\n",
       " 'act',\n",
       " 'actaually',\n",
       " 'actaully',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actionaccepting',\n",
       " 'actioner',\n",
       " 'actioners',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activation',\n",
       " 'activations',\n",
       " 'active',\n",
       " 'actively',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = count_vect.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:28:11.804475Z",
     "start_time": "2019-06-17T02:28:10.978502Z"
    },
    "id": "_8dIDyHjLtDf",
    "outputId": "ea0614e2-66f1-4ddd-bff0-142cfd4fd78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Top Words\n",
      "----- --------------------------------------------------------------------------------\n",
      "    0 effects max special mp3 force office van finding catholic brando\n",
      "    1 memory error zen diabetes sharp blocks attractive control violence diet\n",
      "    2 manson rice haiku bugliosi cooker studies member wayne references charles\n",
      "    3 scent exuviance varies completly duo qing gopichand bela llamas males\n",
      "    4 economics japanese government scared theme publisher moment literature wanting higgins\n",
      "    5 edition hollywood versions diane lane worthy tales gay drivel paperback\n",
      "    6 richard emma stockings schure wagner questo prayer strauss è draft\n",
      "    7 cave shame food winston bear ayla clan exam cooking alive\n",
      "    8 boots these weight pair boot shoes anywhere source them smooth\n",
      "    9 i the it to and a my for not this\n",
      "   10 costs cash hip label et mill hop funk slip delicious\n",
      "   11 the and a of i to this is it in\n",
      "   12 thrash steer nuclear yea dede voivod gillain obtain funniest fifty\n",
      "   13 lame cat descent suspense professional lord creepy henry tomcat structure\n",
      "   14 christ minds jesus cop souls titan elvis sooo criticize lonely\n",
      "   15 sea pratchett discworld fabric gluten mask buddy union paris clue\n",
      "   16 fit ear jawbone la de y en fits humanity el\n",
      "   17 gothic lighter scooter christine scheme wells pool coloring geforce shoot\n",
      "   18 cd music album songs song listen tracks great their print\n",
      "   19 mad stargate vocals items puzzle timeless rod chose keel rip\n"
     ]
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dissect function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05       0.05       0.05000001 ... 0.05       0.05       0.05      ]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05       0.05       0.05       ... 0.05       0.05000001 0.05      ]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05       0.05       0.05       ... 0.05       0.05000001 0.05      ]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[22.30335756  0.05000001  0.05000003 ...  0.05        0.05\n",
      "  0.05      ]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[5.70724328e+01 3.81595565e+01 2.43159917e+01 ... 5.00000002e-02\n",
      " 5.00000004e-02 5.00000006e-02]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05       0.05       0.05       ... 0.05       1.20707175 1.0485002 ]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n",
      "[0.05000001 0.05       0.05       ... 0.05       0.05       0.05      ]\n",
      "[0.05 0.05 0.05 ... 0.05 0.05 0.05]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_dist in enumerate(topic_word): # iterate through 20 rows\n",
    "    print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important line\n",
    "topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1] # [start:stop:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '00', '000', ..., 'éviter', 'última', 'única'], dtype='<U76')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'única'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.05, 0.05, ..., 0.05, 0.05, 0.05])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.05, 0.05, ..., 0.05, 0.05, 0.05])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.argsort()`: Returns the indices that would sort an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13548, 20623, 16659, ..., 26301, 17532,  9391],\n",
       "       [18250,  8527, 14694, ..., 31578,  9990, 17768],\n",
       "       [26944, 29562,  7981, ..., 12927, 23796, 17282],\n",
       "       ...,\n",
       "       [ 2695, 31564, 10022, ..., 24651, 16516, 12496],\n",
       "       [24503, 22420,  8024, ...,  1435, 18652,  4928],\n",
       "       [24699,  6364, 19033, ..., 30378, 26655, 17073]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13548, 20623, 16659, ..., 26301, 17532,  9391], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(topic_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13548"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(topic_word[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['effects', 'max', 'special', 'mp3', 'force', 'office', 'van',\n",
       "       'finding', 'catholic', 'brando'], dtype='<U76')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vocab)[np.argsort(topic_word[0])][:-11:-1] # step -1 reverses the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['effects max special mp3 force office van finding catholic brando',\n",
       " 'memory error zen diabetes sharp blocks attractive control violence diet',\n",
       " 'manson rice haiku bugliosi cooker studies member wayne references charles',\n",
       " 'scent exuviance varies completly duo qing gopichand bela llamas males',\n",
       " 'economics japanese government scared theme publisher moment literature wanting higgins',\n",
       " 'edition hollywood versions diane lane worthy tales gay drivel paperback',\n",
       " 'richard emma stockings schure wagner questo prayer strauss è draft',\n",
       " 'cave shame food winston bear ayla clan exam cooking alive',\n",
       " 'boots these weight pair boot shoes anywhere source them smooth',\n",
       " 'i the it to and a my for not this',\n",
       " 'costs cash hip label et mill hop funk slip delicious',\n",
       " 'the and a of i to this is it in',\n",
       " 'thrash steer nuclear yea dede voivod gillain obtain funniest fifty',\n",
       " 'lame cat descent suspense professional lord creepy henry tomcat structure',\n",
       " 'christ minds jesus cop souls titan elvis sooo criticize lonely',\n",
       " 'sea pratchett discworld fabric gluten mask buddy union paris clue',\n",
       " 'fit ear jawbone la de y en fits humanity el',\n",
       " 'gothic lighter scooter christine scheme wells pool coloring geforce shoot',\n",
       " 'cd music album songs song listen tracks great their print',\n",
       " 'mad stargate vocals items puzzle timeless rod chose keel rip']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtfnK1jeLtDl"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:12.273365Z",
     "start_time": "2019-06-17T02:34:12.263393Z"
    },
    "id": "uwVaWSyTLtDm"
   },
   "outputs": [],
   "source": [
    "## helper function\n",
    "def train_model(classifier, X_train, y_train, X_test):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    return accuracy_score(predictions, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! `y_test` was not specifically imported into the function. If the variable is defined in the global scope (i.e., outside of any function), then it can be accessed from within any function without needing to pass it in as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:14.900001Z",
     "start_time": "2019-06-17T02:34:14.894016Z"
    },
    "id": "f_onpqUkLtDo"
   },
   "outputs": [],
   "source": [
    "# compile results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXwLriDpLtDq"
   },
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:34.147043Z",
     "start_time": "2019-06-17T02:34:34.123096Z"
    },
    "id": "ZcU6IKyNLtDs",
    "outputId": "d2defcfb-2046-47e1-b3b6-08d6edca94f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors    : 0.8540\n",
      "\n",
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print('NB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:36.399812Z",
     "start_time": "2019-06-17T02:34:36.381861Z"
    },
    "id": "zqEG_ByTLtDv",
    "outputId": "0ba74f49-a71c-4d59-f57b-f39f546241ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF : 0.8600\n",
      "\n",
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:39.076000Z",
     "start_time": "2019-06-17T02:34:39.059047Z"
    },
    "id": "uKEEPNi8LtDy",
    "outputId": "c00cf532-c914-4670-cde5-69bf54bf7201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors   : 0.8400\n",
      "\n",
      "Wall time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:42.057019Z",
     "start_time": "2019-06-17T02:34:42.009151Z"
    },
    "id": "M9aIibkBLtD0",
    "outputId": "1f52c1f4-150d-4195-f927-cb66ec41baba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors: 0.8180\n",
      "\n",
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:46.265712Z",
     "start_time": "2019-06-17T02:34:46.258734Z"
    },
    "id": "kkrodUzCLtD3"
   },
   "outputs": [],
   "source": [
    "results.loc['Naïve Bayes'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2oNfajULtD4"
   },
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:50.841687Z",
     "start_time": "2019-06-17T02:34:48.637032Z"
    },
    "id": "OFBhhPZ6LtD4",
    "outputId": "cb5a2b19-dfc1-4b11-e698-0488ba4eae53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors    : 0.8520\n",
      "\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n",
    "print('LR, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.310433Z",
     "start_time": "2019-06-17T02:34:51.214690Z"
    },
    "id": "C89hRhDiLtD6",
    "outputId": "023669b1-788f-4a1b-d282-99464e26312c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF : 0.8730\n",
      "\n",
      "Wall time: 86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.749258Z",
     "start_time": "2019-06-17T02:34:51.683435Z"
    },
    "id": "MvhV1jC6LtD9",
    "outputId": "9bef9b1d-2dc3-4300-c4ff-831957aadee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors   : 0.8360\n",
      "\n",
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:52.635899Z",
     "start_time": "2019-06-17T02:34:52.175122Z"
    },
    "id": "XPjIxmtKLtEA",
    "outputId": "58f4b0e9-e786-45a1-830e-5945129792d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors: 0.8485\n",
      "\n",
      "Wall time: 395 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:53.029844Z",
     "start_time": "2019-06-17T02:34:53.018872Z"
    },
    "id": "ZFK_LWTcLtED"
   },
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1wYto68LtEE"
   },
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.237613Z",
     "start_time": "2019-06-17T02:34:53.406835Z"
    },
    "id": "yYGz8he5LtEE",
    "outputId": "546b1ea8-27c9-45bb-fd91-da08244d6796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors    : 0.8345\n",
      "\n",
      "Wall time: 720 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.743263Z",
     "start_time": "2019-06-17T02:34:54.606629Z"
    },
    "id": "wMt26K0cLtEG",
    "outputId": "406cf62d-c09d-4f9b-c298-eabce0b69238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF : 0.8610\n",
      "\n",
      "Wall time: 70 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:55.220003Z",
     "start_time": "2019-06-17T02:34:55.119256Z"
    },
    "id": "eFt6Y1VvLtEI",
    "outputId": "628299db-8bca-4698-c64e-5b291bd248e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors   : 0.8210\n",
      "\n",
      "Wall time: 49 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.139528Z",
     "start_time": "2019-06-17T02:34:55.585010Z"
    },
    "id": "iqhsS579LtEL",
    "outputId": "7cc3d723-9858-43a8-dcdf-37fcbef9456e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors: 0.8570\n",
      "\n",
      "Wall time: 426 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.501558Z",
     "start_time": "2019-06-17T02:34:56.492592Z"
    },
    "id": "go0bcKeILtEN"
   },
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLGxWK0yLtEO"
   },
   "source": [
    "### Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:20.807157Z",
     "start_time": "2019-06-17T02:34:56.823697Z"
    },
    "id": "HR8aOytWLtEO",
    "outputId": "961d5bca-c1fe-46be-ba8c-2f6325d85901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors    : 0.8375\n",
      "\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n",
    "print('RF, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:30.127233Z",
     "start_time": "2019-06-17T02:35:21.198110Z"
    },
    "id": "zXcTCTEGLtET",
    "outputId": "e4336045-c508-4372-e102-35c559f9a195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF : 0.8315\n",
      "\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:40.162390Z",
     "start_time": "2019-06-17T02:35:30.607944Z"
    },
    "id": "EnvT8qvSLtEW",
    "outputId": "0c3a8fa0-7ee7-40ad-dc0e-85a8c3995733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors   : 0.7810\n",
      "\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:07.640904Z",
     "start_time": "2019-06-17T02:35:40.542371Z"
    },
    "id": "8jt-dTVELtEX",
    "outputId": "dc1113c9-b3f2-4d2e-f4d7-a5bcc98a020c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, CharLevel Vectors: 0.7885\n",
      "\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:08.104108Z",
     "start_time": "2019-06-17T02:36:08.097127Z"
    },
    "id": "fVKeCH_VLtEZ"
   },
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyVz4Q6ILtEa"
   },
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:37.197296Z",
     "start_time": "2019-06-17T02:36:08.451184Z"
    },
    "id": "8wGvHTg-LtEb",
    "outputId": "dcc31f90-b0f8-4f5d-c835-25be80638e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, Count Vectors    : 0.7990\n",
      "\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n",
    "print('GB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:52.852454Z",
     "start_time": "2019-06-17T02:36:37.714920Z"
    },
    "id": "HJNwQf57LtEd",
    "outputId": "686abbc5-8745-4dca-c2e2-b665b9d19901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, WordLevel TF-IDF : 0.7955\n",
      "\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:37:02.608379Z",
     "start_time": "2019-06-17T02:36:53.252355Z"
    },
    "id": "iyPqLMgkLtEe",
    "outputId": "b7c87e04-f724-4ccf-e02e-28beb7e40654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, N-Gram Vectors   : 0.7370\n",
      "\n",
      "Wall time: 8.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.314194Z",
     "start_time": "2019-06-17T02:37:03.039224Z"
    },
    "id": "1KYdyatTLtEg",
    "outputId": "aa4f303e-76ff-47e6-a04f-7e311e1848ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, CharLevel Vectors: 0.8020\n",
      "\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.683222Z",
     "start_time": "2019-06-17T02:39:14.675213Z"
    },
    "id": "AC0hWO59LtEj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:15.024279Z",
     "start_time": "2019-06-17T02:39:15.010319Z"
    },
    "id": "b9fY4J7XLtEk",
    "outputId": "ea4a8ddf-5b37-4cb9-da79-1920e1967b03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Vectors</th>\n",
       "      <th>WordLevel TF-IDF</th>\n",
       "      <th>N-Gram Vectors</th>\n",
       "      <th>CharLevel Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naïve Bayes</th>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.8180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.8485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.7885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7955</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.8020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n",
       "Naïve Bayes                    0.8540            0.8600           0.840   \n",
       "Logistic Regression            0.8520            0.8730           0.836   \n",
       "Support Vector Machine         0.8345            0.8610           0.821   \n",
       "Random Forest                  0.8375            0.8315           0.781   \n",
       "Gradient Boosting              0.7990            0.7955           0.737   \n",
       "\n",
       "                        CharLevel Vectors  \n",
       "Naïve Bayes                        0.8180  \n",
       "Logistic Regression                0.8485  \n",
       "Support Vector Machine             0.8570  \n",
       "Random Forest                      0.7885  \n",
       "Gradient Boosting                  0.8020  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2023 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Pck1cuvLtDH",
    "mQCAUFWYLtDb",
    "OXwLriDpLtDq",
    "-2oNfajULtD4",
    "q1wYto68LtEE",
    "gLGxWK0yLtEO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
